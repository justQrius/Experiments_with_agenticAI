{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AutoGen Core: Distributed Runtime with gRPC\n",
                "\n",
                "This notebook demonstrates **distributed agent execution** using AutoGen Core's gRPC-based runtime. Unlike standalone runtimes (which run all agents in a single process), distributed runtimes enable:\n",
                "1. **Horizontal scaling**: Agents run across multiple worker processes\n",
                "2. **Fault isolation**: Agent failures don't crash the entire system\n",
                "3. **Resource optimization**: Different agents can run on different hardware\n",
                "\n",
                "**Architecture:** A central host coordinates message routing between distributed worker runtimes, each hosting one or more agents."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import dependencies\n",
                "from dataclasses import dataclass\n",
                "from autogen_core import AgentId, MessageContext, RoutedAgent, message_handler\n",
                "from autogen_agentchat.agents import AssistantAgent\n",
                "from autogen_agentchat.messages import TextMessage\n",
                "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
                "from autogen_ext.runtimes.grpc import GrpcWorkerAgentRuntimeHost, GrpcWorkerAgentRuntime\n",
                "from autogen_ext.tools.langchain import LangChainToolAdapter\n",
                "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
                "from langchain.agents import Tool\n",
                "from IPython.display import display, Markdown\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv(override=True)\n",
                "\n",
                "# Runtime Configuration\n",
                "ALL_IN_ONE_WORKER = False  # Set to True for single-worker testing"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 1: Define Message Schema"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class Message:\n",
                "    content: str"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 2: Initialize gRPC Host"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Start gRPC Host (Message Router)\n",
                "host = GrpcWorkerAgentRuntimeHost(address=\"localhost:50051\")\n",
                "host.start()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 3: Configure Tools"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Web Search Tool\n",
                "serper = GoogleSerperAPIWrapper()\n",
                "langchain_serper = Tool(\n",
                "    name=\"internet_search\", \n",
                "    func=serper.run, \n",
                "    description=\"Search the internet\"\n",
                ")\n",
                "autogen_serper = LangChainToolAdapter(langchain_serper)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Task Instructions\n",
                "instruction1 = \"\"\"Research and briefly list the pros of using AutoGen for an AI agent project.\"\"\"\n",
                "instruction2 = \"\"\"Research and briefly list the cons of using AutoGen for an AI agent project.\"\"\"\n",
                "\n",
                "judge_prompt = \"\"\"You must decide whether to use AutoGen for a project based on the following research. \n",
                "Provide your decision and brief rationale.\"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 4: Define Distributed Agents"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Research Agent 1 (Pros)\n",
                "class Player1Agent(RoutedAgent):\n",
                "    def __init__(self, name: str) -> None:\n",
                "        super().__init__(name)\n",
                "        model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
                "        self._delegate = AssistantAgent(\n",
                "            name, \n",
                "            model_client=model_client, \n",
                "            tools=[autogen_serper], \n",
                "            reflect_on_tool_use=True\n",
                "        )\n",
                "\n",
                "    @message_handler\n",
                "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
                "        text_message = TextMessage(content=message.content, source=\"user\")\n",
                "        response = await self._delegate.on_messages([text_message], ctx.cancellation_token)\n",
                "        return Message(content=response.chat_message.content)\n",
                "\n",
                "# Research Agent 2 (Cons)\n",
                "class Player2Agent(RoutedAgent):\n",
                "    def __init__(self, name: str) -> None:\n",
                "        super().__init__(name)\n",
                "        model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
                "        self._delegate = AssistantAgent(\n",
                "            name, \n",
                "            model_client=model_client, \n",
                "            tools=[autogen_serper], \n",
                "            reflect_on_tool_use=True\n",
                "        )\n",
                "\n",
                "    @message_handler\n",
                "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
                "        text_message = TextMessage(content=message.content, source=\"user\")\n",
                "        response = await self._delegate.on_messages([text_message], ctx.cancellation_token)\n",
                "        return Message(content=response.chat_message.content)\n",
                "\n",
                "# Judge Agent (Orchestrator)\n",
                "class Judge(RoutedAgent):\n",
                "    def __init__(self, name: str) -> None:\n",
                "        super().__init__(name)\n",
                "        model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
                "        self._delegate = AssistantAgent(name, model_client=model_client)\n",
                "\n",
                "    @message_handler\n",
                "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
                "        # Request research from both agents\n",
                "        response1 = await self.send_message(Message(content=instruction1), AgentId(\"player1\", \"default\"))\n",
                "        response2 = await self.send_message(Message(content=instruction2), AgentId(\"player2\", \"default\"))\n",
                "        \n",
                "        # Synthesize decision\n",
                "        result = f\"## Pros:\\n{response1.content}\\n\\n## Cons:\\n{response2.content}\\n\\n\"\n",
                "        judgement = f\"{judge_prompt}\\n{result}Your decision:\"\n",
                "        text_msg = TextMessage(content=judgement, source=\"user\")\n",
                "        response = await self._delegate.on_messages([text_msg], ctx.cancellation_token)\n",
                "        return Message(content=result + \"\\n\\n## Decision:\\n\\n\" + response.chat_message.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 5: Deploy Agents to Distributed Workers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if ALL_IN_ONE_WORKER:\n",
                "    # Single worker (for testing)\n",
                "    worker = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n",
                "    await worker.start()\n",
                "    await Player1Agent.register(worker, \"player1\", lambda: Player1Agent(\"player1\"))\n",
                "    await Player2Agent.register(worker, \"player2\", lambda: Player2Agent(\"player2\"))\n",
                "    await Judge.register(worker, \"judge\", lambda: Judge(\"judge\"))\n",
                "else:\n",
                "    # Distributed: Each agent on separate worker\n",
                "    worker1 = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n",
                "    await worker1.start()\n",
                "    await Player1Agent.register(worker1, \"player1\", lambda: Player1Agent(\"player1\"))\n",
                "\n",
                "    worker2 = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n",
                "    await worker2.start()\n",
                "    await Player2Agent.register(worker2, \"player2\", lambda: Player2Agent(\"player2\"))\n",
                "\n",
                "    worker = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n",
                "    await worker.start()\n",
                "    await Judge.register(worker, \"judge\", lambda: Judge(\"judge\"))\n",
                "\n",
                "agent_id = AgentId(\"judge\", \"default\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Execute Distributed Workflow"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Trigger Workflow\n",
                "response = await worker.send_message(Message(content=\"Go!\"), agent_id)\n",
                "display(Markdown(response.content))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cleanup\n",
                "await worker.stop()\n",
                "if not ALL_IN_ONE_WORKER:\n",
                "    await worker1.stop()\n",
                "    await worker2.stop()\n",
                "await host.stop()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}