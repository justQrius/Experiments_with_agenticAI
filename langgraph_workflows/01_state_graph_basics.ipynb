{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LangGraph Fundamentals: State Graphs and Message Management\n",
                "\n",
                "This notebook introduces **LangGraph**, a framework for building stateful, cyclic agent workflows. Unlike traditional linear pipelines, LangGraph allows conditional branching, loops, and persistent state across multiple agent interactions.\n",
                "\n",
                "**Core Concepts:**\n",
                "1. **State**: A typed object (Pydantic BaseModel or TypedDict) representing workflow context\n",
                "2. **Nodes**: Python functions that transform state\n",
                "3. **Edges**: Connections between nodes, defining execution flow\n",
                "4. **Reducers**: Functions that merge new state with existing state (e.g., `add_messages`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import dependencies\n",
                "from typing import Annotated\n",
                "from langgraph.graph import StateGraph, START, END\n",
                "from langgraph.graph.message import add_messages\n",
                "from dotenv import load_dotenv\n",
                "from IPython.display import Image, display\n",
                "import gradio as gr\n",
                "from langchain_openai import ChatOpenAI\n",
                "from pydantic import BaseModel\n",
                "import random"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize Environment\n",
                "load_dotenv(override=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Understanding State and Reducers\n",
                "\n",
                "In LangGraph, `Annotated` is used to specify a **reducer function** that determines how new state values are merged with existing state. The built-in `add_messages` reducer appends new messages to a conversation history."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define State Schema\n",
                "class State(BaseModel):\n",
                "    messages: Annotated[list, add_messages]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 1: Simple Non-LLM Graph\n",
                "\n",
                "Demonstrating that LangGraph is framework-agnosticâ€”nodes can be any Python function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test Data\n",
                "nouns = [\"Cabbages\", \"Unicorns\", \"Toasters\", \"Penguins\", \"Bananas\"]\n",
                "adjectives = [\"outrageous\", \"smelly\", \"existential\", \"sparkly\", \"sarcastic\"]\n",
                "\n",
                "# Node Function\n",
                "def random_phrase_node(old_state: State) -> State:\n",
                "    reply = f\"{random.choice(nouns)} are {random.choice(adjectives)}\"\n",
                "    messages = [{\"role\": \"assistant\", \"content\": reply}]\n",
                "    return State(messages=messages)\n",
                "\n",
                "# Build Graph\n",
                "graph_builder = StateGraph(State)\n",
                "graph_builder.add_node(\"phrase_generator\", random_phrase_node)\n",
                "graph_builder.add_edge(START, \"phrase_generator\")\n",
                "graph_builder.add_edge(\"phrase_generator\", END)\n",
                "\n",
                "# Compile\n",
                "graph = graph_builder.compile()\n",
                "display(Image(graph.get_graph().draw_mermaid_png()))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test Execution\n",
                "initial_state = State(messages=[{\"role\": \"user\", \"content\": \"Hello\"}])\n",
                "result = graph.invoke(initial_state)\n",
                "print(result[\"messages\"][-1].content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 2: LLM-Powered Chatbot Graph"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LLM Client\n",
                "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
                "\n",
                "# Chatbot Node\n",
                "def chatbot_node(old_state: State) -> State:\n",
                "    response = llm.invoke(old_state.messages)\n",
                "    return State(messages=[response])\n",
                "\n",
                "# Build Graph\n",
                "graph_builder = StateGraph(State)\n",
                "graph_builder.add_node(\"chatbot\", chatbot_node)\n",
                "graph_builder.add_edge(START, \"chatbot\")\n",
                "graph_builder.add_edge(\"chatbot\", END)\n",
                "\n",
                "# Compile\n",
                "graph = graph_builder.compile()\n",
                "display(Image(graph.get_graph().draw_mermaid_png()))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Interactive Interface\n",
                "def chat(user_input: str, history):\n",
                "    initial_state = State(messages=[{\"role\": \"user\", \"content\": user_input}])\n",
                "    result = graph.invoke(initial_state)\n",
                "    return result['messages'][-1].content\n",
                "\n",
                "gr.ChatInterface(chat, type=\"messages\").launch()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}